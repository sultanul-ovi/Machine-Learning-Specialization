{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Importing Libraries:** Essential Python libraries including NumPy, Matplotlib, and Pandas are imported for handling arrays, plotting graphs, and manipulating datasets respectively.\n",
        "\n",
        "- **Importing the Dataset:** The process of importing a dataset using Pandas is demonstrated, focusing on separating it into independent features (`X`) and the dependent target variable (`y`).\n",
        "\n",
        "- **Splitting the Dataset:** The dataset is split into Training and Test sets using scikit-learn's `train_test_split` function, an essential step for model evaluation.\n",
        "\n",
        "- **Handling Missing Data:** Techniques for handling missing data are covered, highlighting the importance of proper data imputation.\n",
        "\n",
        "- **Encoding Categorical Data:** The notebooks discuss encoding categorical variables to convert them into a machine-readable format, using methods like OneHotEncoder and LabelEncoder.\n",
        "\n",
        "- **Feature Scaling:** Feature scaling methods, especially standardization, are emphasized to ensure all features contribute equally to the model's performance.\n",
        "\n",
        "- **Print Transformed Data:** Finally, the transformed feature sets are printed to verify the effectiveness of the preprocessing steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "N-qiINBQSK2g"
      },
      "outputs": [],
      "source": [
        "import numpy as np # numpy allows us to work with arrays\n",
        "import matplotlib.pyplot as plt #matplot allows us to plot charts\n",
        "import pandas as pd #pandas allows to import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WwEPNDWySTKm"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('data/pre_data.csv')\n",
        "X = dataset.iloc[:, :-1].values # Features or independent variables\n",
        "y = dataset.iloc[:, -1].values # Dependent variable or result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "colab_type": "code",
        "id": "hCsz2yCebe1R",
        "outputId": "1e4cc568-4e51-4b38-9d46-4aa3f15204be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X) #the matrix of features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "eYrOQ43XcJR3",
        "outputId": "e0873b2a-3b08-4bab-ef0d-15b88858ca44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ],
      "source": [
        "print(y) # the dependent variable vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- we will be handling missing data by changing it with the average of all the values, we could have used median, high frequency values, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "c93k7ipkSexq"
      },
      "outputs": [],
      "source": [
        "# Import the SimpleImputer class from scikit-learn for handling missing values.\n",
        "from sklearn.impute import SimpleImputer  \n",
        "# Create an imputer object that replaces missing values with the mean of the remaining values in the column.\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')  \n",
        "# Fit the imputer to the columns of interest (second and third columns) in the dataset to calculate the mean values for each.\n",
        "imputer.fit(X[:, 1:3])  \n",
        "# Transform the dataset by replacing missing values with the calculated means in the specified columns.\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "colab_type": "code",
        "id": "3UgLdMS_bjq_",
        "outputId": "254af4e0-681e-47f5-aaa7-b9c6f43258e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5hwuVddlSwVi"
      },
      "outputs": [],
      "source": [
        "# Import the ColumnTransformer class from scikit-learn to apply transformations to columns of an array or pandas DataFrame.\n",
        "from sklearn.compose import ColumnTransformer\n",
        "# Import the OneHotEncoder class from scikit-learn for encoding categorical features as a one-hot numeric array.\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Create a ColumnTransformer object to apply a OneHotEncoder to the first column of the dataset; leave the rest of the columns unchanged.\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "\n",
        "# Apply the ColumnTransformer to the dataset, converting it to an array, and overwrite the original dataset with the transformed data.\n",
        "X = np.array(ct.fit_transform(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "colab_type": "code",
        "id": "f7QspewyeBfx",
        "outputId": "5b35feef-7fe2-46ef-ce70-80495f94f4ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XgHCShVyTOYY"
      },
      "outputs": [],
      "source": [
        "# Import the LabelEncoder class from scikit-learn for encoding labels with value between 0 and n_classes-1.\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create an instance of the LabelEncoder class.\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit the label encoder and return encoded labels for the target variable y.\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "FyhY8-gPpFCa",
        "outputId": "7f76ef29-5423-4c3e-cf69-45fbc366a997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pXgA6CzlqbCl"
      },
      "outputs": [],
      "source": [
        "# Import the train_test_split function from scikit-learn to split arrays or matrices into random train and test subsets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and testing sets, with 20% of the data used as the test set. \n",
        "# The random_state parameter ensures reproducibility of the split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "colab_type": "code",
        "id": "GuwQhFdKrYTM",
        "outputId": "de1e527f-c229-4daf-e7c5-ea9d2485148d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "TUrX_Tvcrbi4",
        "outputId": "9a041a9b-2642-4828-fa2f-a431d7d77631"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "pSMHiIsWreQY",
        "outputId": "5afe91e0-9244-4bf5-ec1b-e3e092b85c08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "I_tW7H56rgtW",
        "outputId": "2a93f141-2a99-4a69-eec5-c82a3bb8d36b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AxjSUXFQqo-3"
      },
      "outputs": [],
      "source": [
        "# Import the StandardScaler class from scikit-learn for standardizing features by removing the mean and scaling to unit variance.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create an instance of the StandardScaler class.\n",
        "sc = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler to the training data starting from the fourth column onwards and transform these columns.\n",
        "# This standardizes the features by making them have a mean of 0 and a standard deviation of 1.\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
        "\n",
        "# Transform the testing data starting from the fourth column onwards using the same scaler (mean and std) as the training data.\n",
        "# This ensures that the test data is scaled in the same way as the training data.\n",
        "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "colab_type": "code",
        "id": "DWPET8ZdlMnu",
        "outputId": "dea86927-5124-4e2a-e974-2804df9a913c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "sTXykB_QlRjE",
        "outputId": "b68f0cfc-d07c-48cb-80d0-6800028c41f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ]
        }
      ],
      "source": [
        "print(X_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "data_preprocessing_tools.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
